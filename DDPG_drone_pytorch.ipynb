{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Pendulum-v0 environment\n",
    "\n",
    "- No batch normalization\n",
    "- Random Gaussian parameter noise (Not using action noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unity_wrappers import unity_env_generator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"mlagents.envs\")\n",
    "logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Mu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DDPG_Mu, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 512)\n",
    "        self.fc_mu = nn.Linear(512, 3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = torch.tanh(self.fc_mu(x))*2\n",
    "        return mu\n",
    "    \n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "class DDPG_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DDPG_Q, self).__init__()\n",
    "        self.fc_a = nn.Linear(3, 128)\n",
    "        self.fc_s = nn.Linear(9, 128)\n",
    "        self.fc_1 = nn.Linear(256, 256)\n",
    "        self.fc_q = nn.Linear(256, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        x1 = F.relu(self.fc_a(a))\n",
    "        x2 = F.relu(self.fc_s(x))\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        q = self.fc_q(x)\n",
    "        return q\n",
    "\n",
    "    \n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = unity_env_generator(\"Drone\")\n",
    "Q, Q_p, Mu, Mu_p = DDPG_Q(), DDPG_Q(), DDPG_Mu(), DDPG_Mu()\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 30000\n",
    "replay_buffer = []\n",
    "TAU = 0.01\n",
    "PARAMETER_NOISE_COEF = 0.0005\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    for i in range(EPOCH):\n",
    "        s, a, r, s_p, done_mask = make_minibatch()\n",
    "\n",
    "        target = r + GAMMA*Q_p(s_p, Mu_p(s_p))*done_mask\n",
    "        critic_loss = F.smooth_l1_loss(Q(s,a), target.detach())\n",
    "        Q.train(critic_loss)\n",
    "        \n",
    "        actor_loss = -Q(s, Mu(s)).mean()\n",
    "        Mu.train(actor_loss)\n",
    "        \n",
    "        soft_target_update(Mu, Mu_p)\n",
    "        soft_target_update(Q, Q_p)\n",
    "    \n",
    "def soft_target_update(model, model_p):\n",
    "    for param_target, param in zip(model_p.parameters(), model.parameters()):\n",
    "        param_target.data.copy_(param_target.data*(1.0 - TAU) + param.data*TAU)\n",
    "        \n",
    "def init_target_param(model, model_p):\n",
    "    for param_target, param in zip(model_p.parameters(), model.parameters()):\n",
    "        param_target.data.copy_(param.data)\n",
    "        \n",
    "def parameter_noise(model):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.add_(torch.randn(param.size()) * PARAMETER_NOISE_COEF)\n",
    "            \n",
    "def store_transition(s, a, r, s_prime, done):\n",
    "    if len(replay_buffer) == BUFFER_SIZE:\n",
    "        del(replay_buffer[0])\n",
    "    s = s.unsqueeze(0)\n",
    "    a = a.unsqueeze(0)\n",
    "    r = torch.tensor([r], dtype=torch.float).unsqueeze(0)\n",
    "    s_prime = s_prime.unsqueeze(0)\n",
    "    replay_buffer.append((s, a, r, s_prime, done))\n",
    "    \n",
    "def make_minibatch():\n",
    "    s_list, r_list, a_list, s_p_list, done_list = [], [], [], [], []\n",
    "    mini_batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    for sample in mini_batch:\n",
    "        s, a, r, s_p, done = sample\n",
    "        s_list.append(s)\n",
    "        a_list.append(a)\n",
    "        r_list.append(r)\n",
    "        s_p_list.append(s_p)\n",
    "        done_list.append([0]) if done else done_list.append([1])\n",
    "    return torch.cat(s_list, dim=0), torch.cat(a_list, dim=0), torch.cat(r_list, dim=0), torch.cat(s_p_list, dim=0),\\\n",
    "            torch.tensor(done_list, dtype=torch.float).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_sum = 0.0\n",
    "reward_list = []\n",
    "init_target_param(Mu, Mu_p)\n",
    "init_target_param(Q, Q_p)\n",
    "\n",
    "for ep in range(20000):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        state = torch.tensor(observation, dtype=torch.float)\n",
    "        parameter_noise(Mu) ## for exploration\n",
    "        action = Mu(state).detach() ## must .detach!! <- important!!!\n",
    "        observation, reward, done, _ = env.step(action.numpy())\n",
    "        reward_sum += reward\n",
    "        next_state = torch.tensor(observation, dtype=torch.float)\n",
    "        store_transition(state, action, reward, next_state, done)   \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if len(replay_buffer) >= 500:\n",
    "        training()\n",
    "            \n",
    "    if ep % 20 == 19:\n",
    "        print('Episode %d'%ep,', Reward mean : %f'%(reward_sum/20.0))\n",
    "        reward_list.append(reward_sum/20.0)\n",
    "        #plt.plot(reward_list)\n",
    "        #plt.show()\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve\n",
    "\n",
    "mean reward every 20 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        state = torch.tensor(observation, dtype=torch.float)\n",
    "        action = Mu(state)\n",
    "        observation, reward, done, _ = env.step(action.numpy())\n",
    "        if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "- https://github.com/seungeunrho/minimalRL/blob/master/ddpg.py\n",
    "- https://github.com/l5shi/Multi-DDPG-with-parameter-noise/blob/master/Multi_DDPG_with_parameter_noise.ipynb\n",
    "- https://arxiv.org/abs/1706.01905\n",
    "- https://openai.com/blog/better-exploration-with-parameter-noise/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
